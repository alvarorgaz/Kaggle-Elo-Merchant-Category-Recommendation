{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import itertools as it\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"Data/final_data_with_mean_encoding.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lists of features by groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifiers\n",
    "identifiers = ['card_id','train_or_test','first_active_month']\n",
    "\n",
    "# Target\n",
    "target = 'target'\n",
    "\n",
    "# Features from original Train/Test.csv\n",
    "features_train_csv = ['feature_1','feature_2','feature_3','year','month','months_to_2018_02']\n",
    "\n",
    "# Features from all_transactions_merchants_groupby_cards\n",
    "features_transactions = [\n",
    "'month_lag__mean','month_lag__std','month_lag__min','month_lag__max','month_lag__range',\n",
    "'purchase_date__range','purchase_date__days_diff_next_purchase_mean',\n",
    "'purchase_date__days_diff_next_purchase_std','purchase_date_year__mean','purchase_date_year__std',\n",
    "'purchase_date_year__min','purchase_date_year__max','purchase_date_year__range',\n",
    "'purchase_date_month__mean','purchase_date_month__std','purchase_date_month__min',\n",
    "'purchase_date_month__max','purchase_date_month__range','purchase_date_day__mean',\n",
    "'purchase_date_day__std','purchase_date_day__min','purchase_date_day__max','purchase_date_day__range',\n",
    "'purchase_date_hour__mean','purchase_date_hour__std','purchase_date_hour__min','purchase_date_hour__max',\n",
    "'purchase_date_hour__range','purchase_date_days_to_2018_02__mean','purchase_date_days_to_2018_02__std',\n",
    "'purchase_date_days_to_2018_02__min','purchase_date_days_to_2018_02__max',\n",
    "'purchase_date_days_to_2018_02__range','authorized_flag_N__mean','authorized_flag_N__var',\n",
    "'authorized_flag_Y__mean','authorized_flag_Y__var','authorized_flag_-9999__mean',\n",
    "'authorized_flag_-9999__var','category_3_A__mean','category_3_A__var','category_3_B__mean',\n",
    "'category_3_B__var','category_3_C__mean','category_3_C__var','category_3_-9999__mean',\n",
    "'category_3_-9999__var','installments__mean','installments__std','installments__min','installments__max',\n",
    "'installments__range','category_1_N__mean','category_1_N__var','category_1_Y__mean','category_1_Y__var',\n",
    "'category_1_-9999__mean','category_1_-9999__var','merchant_category_id__nunique',\n",
    "'merchant_category_id__mode','subsector_id__nunique','subsector_id__mode','merchant_id__nunique',\n",
    "'merchant_id__mode','purchase_amount__mean','purchase_amount__std','purchase_amount__min',\n",
    "'purchase_amount__max','purchase_amount__range','city_id__nunique','city_id__mode','state_id__nunique',\n",
    "'state_id__mode','category_2_1__mean','category_2_1__var','category_2_2__mean','category_2_2__var',\n",
    "'category_2_3__mean','category_2_3__var','category_2_4__mean','category_2_4__var','category_2_5__mean',\n",
    "'category_2_5__var','category_2_-9999__mean','category_2_-9999__var',\n",
    "'merchant_group_id_merchants__nunique','merchant_group_id_merchants__mode',\n",
    "'merchant_category_id_merchants__nunique','merchant_category_id_merchants__mode',\n",
    "'subsector_id_merchants__nunique','subsector_id_merchants__mode','numerical_1_merchants__mean',\n",
    "'numerical_1_merchants__std','numerical_1_merchants__min','numerical_1_merchants__max',\n",
    "'numerical_1_merchants__range','numerical_2_merchants__mean','numerical_2_merchants__std',\n",
    "'numerical_2_merchants__min','numerical_2_merchants__max','numerical_2_merchants__range',\n",
    "'category_1_merchants_N__mean','category_1_merchants_N__var','category_1_merchants_Y__mean',\n",
    "'category_1_merchants_Y__var','category_1_merchants_-9999__mean','category_1_merchants_-9999__var',\n",
    "'most_recent_sales_range_merchants_A__mean','most_recent_sales_range_merchants_A__var',\n",
    "'most_recent_sales_range_merchants_B__mean','most_recent_sales_range_merchants_B__var',\n",
    "'most_recent_sales_range_merchants_C__mean','most_recent_sales_range_merchants_C__var',\n",
    "'most_recent_sales_range_merchants_D__mean','most_recent_sales_range_merchants_D__var',\n",
    "'most_recent_sales_range_merchants_E__mean','most_recent_sales_range_merchants_E__var',\n",
    "'most_recent_sales_range_merchants_-9999__mean','most_recent_sales_range_merchants_-9999__var',\n",
    "'most_recent_purchases_range_merchants_A__mean','most_recent_purchases_range_merchants_A__var',\n",
    "'most_recent_purchases_range_merchants_B__mean','most_recent_purchases_range_merchants_B__var',\n",
    "'most_recent_purchases_range_merchants_C__mean','most_recent_purchases_range_merchants_C__var',\n",
    "'most_recent_purchases_range_merchants_D__mean','most_recent_purchases_range_merchants_D__var',\n",
    "'most_recent_purchases_range_merchants_E__mean','most_recent_purchases_range_merchants_E__var',\n",
    "'most_recent_purchases_range_merchants_-9999__mean','most_recent_purchases_range_merchants_-9999__var',\n",
    "'avg_sales_lag3_merchants__mean','avg_sales_lag3_merchants__std','avg_sales_lag3_merchants__min',\n",
    "'avg_sales_lag3_merchants__max','avg_sales_lag3_merchants__range','avg_purchases_lag3_merchants__mean',\n",
    "'avg_purchases_lag3_merchants__std','avg_purchases_lag3_merchants__min',\n",
    "'avg_purchases_lag3_merchants__max','avg_purchases_lag3_merchants__range',\n",
    "'active_months_lag3_merchants__mean','active_months_lag3_merchants__std',\n",
    "'active_months_lag3_merchants__min','active_months_lag3_merchants__max',\n",
    "'active_months_lag3_merchants__range','avg_sales_lag6_merchants__mean','avg_sales_lag6_merchants__std',\n",
    "'avg_sales_lag6_merchants__min','avg_sales_lag6_merchants__max','avg_sales_lag6_merchants__range',\n",
    "'avg_purchases_lag6_merchants__mean','avg_purchases_lag6_merchants__std',\n",
    "'avg_purchases_lag6_merchants__min','avg_purchases_lag6_merchants__max',\n",
    "'avg_purchases_lag6_merchants__range','active_months_lag6_merchants__mean',\n",
    "'active_months_lag6_merchants__std','active_months_lag6_merchants__min',\n",
    "'active_months_lag6_merchants__max','active_months_lag6_merchants__range',\n",
    "'avg_sales_lag12_merchants__mean','avg_sales_lag12_merchants__std','avg_sales_lag12_merchants__min',\n",
    "'avg_sales_lag12_merchants__max','avg_sales_lag12_merchants__range',\n",
    "'avg_purchases_lag12_merchants__mean','avg_purchases_lag12_merchants__std',\n",
    "'avg_purchases_lag12_merchants__min','avg_purchases_lag12_merchants__max',\n",
    "'avg_purchases_lag12_merchants__range','active_months_lag12_merchants__mean',\n",
    "'active_months_lag12_merchants__std','active_months_lag12_merchants__min',\n",
    "'active_months_lag12_merchants__max','active_months_lag12_merchants__range','category_4_merchants_N__mean',\n",
    "'category_4_merchants_N__var','category_4_merchants_Y__mean','category_4_merchants_Y__var',\n",
    "'category_4_merchants_-9999__mean','category_4_merchants_-9999__var','city_id_merchants__nunique',\n",
    "'city_id_merchants__mode','state_id_merchants__nunique','state_id_merchants__mode',\n",
    "'category_2_merchants_1__mean','category_2_merchants_1__var','category_2_merchants_2__mean',\n",
    "'category_2_merchants_2__var','category_2_merchants_3__mean','category_2_merchants_3__var',\n",
    "'category_2_merchants_4__mean','category_2_merchants_4__var','category_2_merchants_5__mean',\n",
    "'category_2_merchants_5__var','category_2_merchants_-9999__mean','category_2_merchants_-9999__var']\n",
    "\n",
    "# Features from mean_encoding_all_transactions_merchants_groupby_card\n",
    "features_mean_encoding = [\n",
    "    'authorized_flag__mean_encoded','category_3__mean_encoded','category_1__mean_encoded',\n",
    "    'merchant_category_id__mean_encoded','subsector_id__mean_encoded',\n",
    "    'merchant_id__mean_encoded','city_id__mean_encoded','state_id__mean_encoded',\n",
    "    'category_2__mean_encoded','merchant_group_id_merchants__mean_encoded',\n",
    "    'merchant_category_id_merchants__mean_encoded','subsector_id_merchants__mean_encoded',\n",
    "    'category_1_merchants__mean_encoded','most_recent_sales_range_merchants__mean_encoded',\n",
    "    'most_recent_purchases_range_merchants__mean_encoded','category_4_merchants__mean_encoded',\n",
    "    'city_id_merchants__mean_encoded','state_id_merchants__mean_encoded',\n",
    "    'category_2_merchants__mean_encoded']\n",
    "\n",
    "# Features with 1 unique value that should be removed\n",
    "features_1_unique = [\n",
    "    \"purchase_date_year__min\",\"authorized_flag_-9999__mean\",\"authorized_flag_-9999__var\",\n",
    "    \"category_1_-9999__mean\",\"category_1_-9999__var\",\"active_months_lag3_merchants__max\",\n",
    "    \"active_months_lag6_merchants__max\"]\n",
    "\n",
    "# Features mode of IDs\n",
    "features_numeric_IDs_mode = [\n",
    "    'merchant_category_id__mode','subsector_id__mode','city_id__mode',\n",
    "    'state_id__mode','merchant_group_id_merchants__mode','merchant_category_id_merchants__mode',\n",
    "    'subsector_id_merchants__mode','city_id_merchants__mode','state_id_merchants__mode']\n",
    "\n",
    "features_categorical_IDs_mode = ['merchant_id__mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. For XGBRegressor data must be int, float or bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify features type category and object except *merchant_id__mode* which is the unique ID feature no numerical, and we will simply not include it in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_modify = [\"feature_1\",\"feature_2\",\"feature_3\"]+features_numeric_IDs_mode\n",
    "for col in features_to_modify:\n",
    "    data[col] = data[col].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Select features for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will exclude features with 1 unique value and *merchant_id__mode*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((features_train_csv,features_transactions,features_mean_encoding))\n",
    "features = [col for col in features if col not in features_1_unique+features_categorical_IDs_mode]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Target metric: RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = lambda predictions,target: np.mean((predictions-target)**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split Train into 2 validation folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First separate Train & Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.loc[data[\"train_or_test\"]==\"train\",:]\n",
    "test = data.loc[data[\"train_or_test\"]==\"test\",:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then split Train into 2 validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folds(data,k=2,seed=1):\n",
    "    np.random.seed(seed)\n",
    "    data = data.iloc[np.random.permutation(data.index),:]\n",
    "    data_folds_list = [data.iloc[int(data.shape[0]*i/k):int(data.shape[0]*(i+1)/k),:] for i in range(k)]\n",
    "    return(data_folds_list)\n",
    "\n",
    "k = 2\n",
    "train_folds_list = folds(train,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Validation with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all parameters combinations for grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'XGB': XGBRegressor,\n",
    "}\n",
    "\n",
    "parameters_names = {\n",
    "    'XGB': ['max_depth','learning_rate','n_estimators',\"booster\",\"n_jobs\",\"min_child_weight\",\"subsample\",\n",
    "            \"colsample_bytree\",\"random_state\",\"missing\"],\n",
    "}\n",
    "\n",
    "parameters_values = {\n",
    "    # XGB: max_depth, learning_rate, n_estimators, booster, n_jobs, min_child_weight, subsample,\n",
    "    #      colsample_bytree, random_state, missing\n",
    "    'XGB': [[5,10,15,20,25],\n",
    "            [0.01],\n",
    "            [25,50,100,250,500],\n",
    "            [\"gbtree\",\"gblinear\"],\n",
    "            [6],\n",
    "            [10,20,30,40],\n",
    "            [0.75,1],\n",
    "            [0.75,1],\n",
    "            [1],\n",
    "            [-9999]\n",
    "           ],\n",
    "}\n",
    "\n",
    "grid = {}\n",
    "for model in models.keys():\n",
    "    grid[model] = list(it.product(*parameters_values[model]))\n",
    "    for idx in range(len(grid[model])):\n",
    "        arguments = {}\n",
    "        for parameter_index, parameter_value in enumerate(grid[model][idx]):\n",
    "            arguments[parameters_names[model][parameter_index]] = parameter_value\n",
    "        grid[model][idx] = arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search: calulate the RMSE in validation splits for all parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RMSEs = {}\n",
    "for model in models.keys():\n",
    "    RMSEs[model] = pd.DataFrame(data=None,columns=parameters_names[model]+[\"Fold_\"+str(i) for i in \\\n",
    "                                                                           range(k)],[\"Mean\",\"Std\"])\n",
    "    for idx in range(len(grid[model])):\n",
    "        for fold in range(k):\n",
    "            # Define train and validation data for the ith fold\n",
    "            data_train = pd.concat([train_folds_list[i] for i in range(k) if i!=fold],axis=0)\n",
    "            data_val = train_folds_list[fold]        \n",
    "            x_train, y_train = data_train[features], data_train[target]\n",
    "            x_validation, y_validation = data_val[features], data_val[target]\n",
    "            # Train model\n",
    "            print(\"Parametrization \"+str(idx)+\" for Fold_\"+str(fold)+\" starts at \",\n",
    "                  datetime.datetime.now())\n",
    "            regressor = models[model](**grid[model][idx])\n",
    "            regressor.fit(x_train,y_train)\n",
    "            rmse_validation = rmse(regressor.predict(x_validation),y_validation)\n",
    "            print(\"and finishes at \",datetime.datetime.now(),\" with RMSE \",rmse_validation,\"\\n\")\n",
    "            # Add the RMSE to results\n",
    "            RMSEs[model].loc[idx,parameters_names[model]] = grid[model][idx]\n",
    "            RMSEs[model].loc[idx,\"Fold_\"+str(fold)] = rmse_validation\n",
    "        # Add the mean and std of the parametrization RMSE\n",
    "        RMSEs[model].loc[idx,\"Mean\"] = np.mean(RMSEs[model].loc[idx,[\"Fold_\"+str(i) for i in range(k)]])\n",
    "        RMSEs[model].loc[idx,\"Std\"] = np.std(RMSEs[model].loc[idx,[\"Fold_\"+str(i) for i in range(k)]])\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the validation results in a *pickle* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the results by RMSE mean\n",
    "RMSEs[model] = RMSEs[model].loc[np.argsort(RMSEs[model][\"Mean\"]),:]\n",
    "pickle.dump(RMSEs,open('Validations/Results_validation_XGBoost.dat','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train optimal model with all Train and create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs = pd.read_pickle(\"Validations/Results_validation_XGBoost.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>booster</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>random_state</th>\n",
       "      <th>missing</th>\n",
       "      <th>Fold_0</th>\n",
       "      <th>Fold_1</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>2.76425</td>\n",
       "      <td>2.67121</td>\n",
       "      <td>2.71773</td>\n",
       "      <td>0.0465233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>2.76705</td>\n",
       "      <td>2.6744</td>\n",
       "      <td>2.72072</td>\n",
       "      <td>0.0463221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>2.76883</td>\n",
       "      <td>2.67314</td>\n",
       "      <td>2.72098</td>\n",
       "      <td>0.0478485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>2.76894</td>\n",
       "      <td>2.67455</td>\n",
       "      <td>2.72175</td>\n",
       "      <td>0.0471922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>2.77346</td>\n",
       "      <td>2.67827</td>\n",
       "      <td>2.72587</td>\n",
       "      <td>0.0475953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth learning_rate n_estimators booster n_jobs min_child_weight  \\\n",
       "457        15          0.01          500  gbtree      6               30   \n",
       "461        15          0.01          500  gbtree      6               40   \n",
       "613        20          0.01          500  gbtree      6               20   \n",
       "453        15          0.01          500  gbtree      6               20   \n",
       "297        10          0.01          500  gbtree      6               30   \n",
       "\n",
       "    subsample colsample_bytree random_state missing   Fold_0   Fold_1  \\\n",
       "457      0.75                1            1   -9999  2.76425  2.67121   \n",
       "461      0.75                1            1   -9999  2.76705   2.6744   \n",
       "613      0.75                1            1   -9999  2.76883  2.67314   \n",
       "453      0.75                1            1   -9999  2.76894  2.67455   \n",
       "297      0.75                1            1   -9999  2.77346  2.67827   \n",
       "\n",
       "        Mean        Std  \n",
       "457  2.71773  0.0465233  \n",
       "461  2.72072  0.0463221  \n",
       "613  2.72098  0.0478485  \n",
       "453  2.72175  0.0471922  \n",
       "297  2.72587  0.0475953  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSEs[\"XGB\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submission_XGBoost_parametrization#.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all chosen parametrizations with all Train data. *Note:* We will use the 5 parametrizations with lowest mean RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"XGB\"\n",
    "parametrization_indexes = RMSEs[model].index[0:5]\n",
    "# Define train data\n",
    "x_train, y_train = train[features], train[target]\n",
    "# Train model for all chosen parametrizations\n",
    "for idx in parametrization_indexes:\n",
    "    print(\"Parametrization \"+str(idx)+\" with all Train starts at \",datetime.datetime.now())\n",
    "    regressor = models[model](**grid[model][idx])\n",
    "    regressor.fit(x_train,y_train)\n",
    "    print(\"and finishes at \",datetime.datetime.now(),\"\\n\")\n",
    "    # Save model\n",
    "    pickle.dump(regressor,open('Models/Model_XGBoost_parametrization'+str(idx)+'.dat','wb'))\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Test data and create *.csv* file for submitting to Kaggle for all chosen parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of card_id in submission.csv is OK? True\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('Data/sample_submission.csv')\n",
    "print(\"Order of card_id in submission.csv is OK?\",np.mean(submission[\"card_id\"].values==test[\"card_id\"].\\\n",
    "                                                          values)==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametrization_indexes = RMSEs[model].index[0:5]\n",
    "x_test = test[features]\n",
    "for idx in parametrization_indexes:\n",
    "    regressor = pickle.load(open('Models/Model_XGBoost_parametrization'+str(idx)+'.dat','rb'))\n",
    "    submission[\"target\"] = regressor.predict(x_test)\n",
    "    submission.to_csv('Submissions/Submission_XGBoost_parametrization'+str(idx)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submission_XGBoost_parametrization_wo_mean_encoding.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try doing feature selection, excluding the mean encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_wo_mean_encoding = [col for col in features if col not in features_mean_encoding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all chosen parametrizations with all Train data. *Note:* We will use the 5 parametrizations with lowest mean RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"XGB\"\n",
    "parametrization_indexes = RMSEs[model].index[0:5]\n",
    "# Define train data\n",
    "x_train, y_train = train[features_wo_mean_encoding], train[target]\n",
    "# Train model for all chosen parametrizations\n",
    "for idx in parametrization_indexes:\n",
    "    print(\"Parametrization \"+str(idx)+\" with all Train starts at \",datetime.datetime.now())\n",
    "    regressor = models[model](**grid[model][idx])\n",
    "    regressor.fit(x_train,y_train)\n",
    "    print(\"and finishes at \",datetime.datetime.now(),\"\\n\")\n",
    "    # Save model\n",
    "    pickle.dump(regressor,open('Models/Model_XGBoost_parametrization'+str(idx)+'_wo_mean_encoding.dat',\n",
    "                               'wb'))\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Test data and create *.csv* file for submitting to Kaggle for all chosen parametrizations without mean encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of card_id in submission.csv OK? True\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('Data/sample_submission.csv')\n",
    "print(\"Order of card_id in submission.csv OK?\",np.mean(submission[\"card_id\"].values==test[\"card_id\"].\\\n",
    "                                                       values)==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test[features_wo_mean_encoding]\n",
    "for idx in parametrization_indexes:\n",
    "    regressor = pickle.load(open('Models/Model_XGBoost_parametrization'+str(idx)+'_wo_mean_encoding.dat',\n",
    "                                 'rb'))\n",
    "    submission[\"target\"] = regressor.predict(x_test)\n",
    "    submission.to_csv('Submissions/Submission_XGBoost_parametrization'+str(idx)+'_wo_mean_encoding.csv',\n",
    "                      index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
