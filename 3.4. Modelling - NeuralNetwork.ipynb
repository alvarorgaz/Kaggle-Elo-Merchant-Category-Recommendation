{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import itertools as it\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"Data/final_data_with_mean_encoding.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lists of features by groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifiers\n",
    "identifiers = ['card_id','train_or_test','first_active_month']\n",
    "\n",
    "# Target\n",
    "target = 'target'\n",
    "\n",
    "# Features from original Train/Test.csv\n",
    "features_train_csv = ['feature_1','feature_2','feature_3','year','month','months_to_2018_02']\n",
    "\n",
    "# Features from all_transactions_with_merchants_groupby_cards\n",
    "features_transactions = [\n",
    "    'month_lag__mean','month_lag__std','month_lag__min','month_lag__max','month_lag__range',\n",
    "    'purchase_date__range',\n",
    "    'purchase_date__days_diff_next_purchase_mean','purchase_date__days_diff_next_purchase_std',\n",
    "    'purchase_date_year__mean','purchase_date_year__std','purchase_date_year__min','purchase_date_year__max','purchase_date_year__range',\n",
    "    'purchase_date_month__mean','purchase_date_month__std','purchase_date_month__min','purchase_date_month__max','purchase_date_month__range',\n",
    "    'purchase_date_day__mean','purchase_date_day__std','purchase_date_day__min','purchase_date_day__max','purchase_date_day__range',\n",
    "    'purchase_date_hour__mean','purchase_date_hour__std','purchase_date_hour__min','purchase_date_hour__max','purchase_date_hour__range',\n",
    "    'purchase_date_days_to_2018_02__mean','purchase_date_days_to_2018_02__std','purchase_date_days_to_2018_02__min','purchase_date_days_to_2018_02__max','purchase_date_days_to_2018_02__range',\n",
    "    'authorized_flag_N__mean','authorized_flag_N__var',\n",
    "    'authorized_flag_Y__mean','authorized_flag_Y__var',\n",
    "    'authorized_flag_-9999__mean','authorized_flag_-9999__var',\n",
    "    'category_3_A__mean','category_3_A__var',\n",
    "    'category_3_B__mean','category_3_B__var',\n",
    "    'category_3_C__mean','category_3_C__var',\n",
    "    'category_3_-9999__mean','category_3_-9999__var',\n",
    "    'installments__mean','installments__std','installments__min','installments__max','installments__range',\n",
    "    'category_1_N__mean','category_1_N__var',\n",
    "    'category_1_Y__mean','category_1_Y__var',\n",
    "    'category_1_-9999__mean','category_1_-9999__var',\n",
    "    'merchant_category_id__nunique','merchant_category_id__mode',\n",
    "    'subsector_id__nunique','subsector_id__mode',\n",
    "    'merchant_id__nunique','merchant_id__mode',\n",
    "    'purchase_amount__mean','purchase_amount__std','purchase_amount__min','purchase_amount__max','purchase_amount__range',\n",
    "    'city_id__nunique','city_id__mode',\n",
    "    'state_id__nunique','state_id__mode',\n",
    "    'category_2_1__mean','category_2_1__var',\n",
    "    'category_2_2__mean','category_2_2__var',\n",
    "    'category_2_3__mean','category_2_3__var',\n",
    "    'category_2_4__mean','category_2_4__var',\n",
    "    'category_2_5__mean','category_2_5__var',\n",
    "    'category_2_-9999__mean','category_2_-9999__var',\n",
    "    'merchant_group_id_merchants__nunique','merchant_group_id_merchants__mode',\n",
    "    'merchant_category_id_merchants__nunique','merchant_category_id_merchants__mode',\n",
    "    'subsector_id_merchants__nunique','subsector_id_merchants__mode',\n",
    "    'numerical_1_merchants__mean','numerical_1_merchants__std','numerical_1_merchants__min','numerical_1_merchants__max','numerical_1_merchants__range',\n",
    "    'numerical_2_merchants__mean','numerical_2_merchants__std','numerical_2_merchants__min','numerical_2_merchants__max','numerical_2_merchants__range',\n",
    "    'category_1_merchants_N__mean','category_1_merchants_N__var',\n",
    "    'category_1_merchants_Y__mean','category_1_merchants_Y__var',\n",
    "    'category_1_merchants_-9999__mean','category_1_merchants_-9999__var',\n",
    "    'most_recent_sales_range_merchants_A__mean','most_recent_sales_range_merchants_A__var',\n",
    "    'most_recent_sales_range_merchants_B__mean','most_recent_sales_range_merchants_B__var',\n",
    "    'most_recent_sales_range_merchants_C__mean','most_recent_sales_range_merchants_C__var',\n",
    "    'most_recent_sales_range_merchants_D__mean','most_recent_sales_range_merchants_D__var',\n",
    "    'most_recent_sales_range_merchants_E__mean','most_recent_sales_range_merchants_E__var',\n",
    "    'most_recent_sales_range_merchants_-9999__mean','most_recent_sales_range_merchants_-9999__var',\n",
    "    'most_recent_purchases_range_merchants_A__mean','most_recent_purchases_range_merchants_A__var',\n",
    "    'most_recent_purchases_range_merchants_B__mean','most_recent_purchases_range_merchants_B__var',\n",
    "    'most_recent_purchases_range_merchants_C__mean','most_recent_purchases_range_merchants_C__var',\n",
    "    'most_recent_purchases_range_merchants_D__mean','most_recent_purchases_range_merchants_D__var',\n",
    "    'most_recent_purchases_range_merchants_E__mean','most_recent_purchases_range_merchants_E__var',\n",
    "    'most_recent_purchases_range_merchants_-9999__mean','most_recent_purchases_range_merchants_-9999__var',\n",
    "    'avg_sales_lag3_merchants__mean','avg_sales_lag3_merchants__std','avg_sales_lag3_merchants__min','avg_sales_lag3_merchants__max','avg_sales_lag3_merchants__range',\n",
    "    'avg_purchases_lag3_merchants__mean','avg_purchases_lag3_merchants__std','avg_purchases_lag3_merchants__min','avg_purchases_lag3_merchants__max','avg_purchases_lag3_merchants__range',\n",
    "    'active_months_lag3_merchants__mean','active_months_lag3_merchants__std','active_months_lag3_merchants__min','active_months_lag3_merchants__max','active_months_lag3_merchants__range',\n",
    "    'avg_sales_lag6_merchants__mean','avg_sales_lag6_merchants__std','avg_sales_lag6_merchants__min','avg_sales_lag6_merchants__max','avg_sales_lag6_merchants__range',\n",
    "    'avg_purchases_lag6_merchants__mean','avg_purchases_lag6_merchants__std','avg_purchases_lag6_merchants__min','avg_purchases_lag6_merchants__max','avg_purchases_lag6_merchants__range',\n",
    "    'active_months_lag6_merchants__mean','active_months_lag6_merchants__std','active_months_lag6_merchants__min','active_months_lag6_merchants__max','active_months_lag6_merchants__range',\n",
    "    'avg_sales_lag12_merchants__mean','avg_sales_lag12_merchants__std','avg_sales_lag12_merchants__min','avg_sales_lag12_merchants__max','avg_sales_lag12_merchants__range',\n",
    "    'avg_purchases_lag12_merchants__mean','avg_purchases_lag12_merchants__std','avg_purchases_lag12_merchants__min','avg_purchases_lag12_merchants__max','avg_purchases_lag12_merchants__range',\n",
    "    'active_months_lag12_merchants__mean','active_months_lag12_merchants__std','active_months_lag12_merchants__min','active_months_lag12_merchants__max','active_months_lag12_merchants__range',\n",
    "    'category_4_merchants_N__mean','category_4_merchants_N__var',\n",
    "    'category_4_merchants_Y__mean','category_4_merchants_Y__var',\n",
    "    'category_4_merchants_-9999__mean','category_4_merchants_-9999__var',\n",
    "    'city_id_merchants__nunique','city_id_merchants__mode',\n",
    "    'state_id_merchants__nunique','state_id_merchants__mode',\n",
    "    'category_2_merchants_1__mean','category_2_merchants_1__var',\n",
    "    'category_2_merchants_2__mean','category_2_merchants_2__var',\n",
    "    'category_2_merchants_3__mean','category_2_merchants_3__var',\n",
    "    'category_2_merchants_4__mean','category_2_merchants_4__var',\n",
    "    'category_2_merchants_5__mean','category_2_merchants_5__var',\n",
    "    'category_2_merchants_-9999__mean','category_2_merchants_-9999__var']\n",
    "\n",
    "# Features from mean_encoding_all_transactions_with_merchants_groupby_card\n",
    "features_mean_encoding = [\n",
    "    'authorized_flag__mean_encoded','category_3__mean_encoded','category_1__mean_encoded',\n",
    "    'merchant_category_id__mean_encoded','subsector_id__mean_encoded',\n",
    "    'merchant_id__mean_encoded','city_id__mean_encoded','state_id__mean_encoded',\n",
    "    'category_2__mean_encoded','merchant_group_id_merchants__mean_encoded',\n",
    "    'merchant_category_id_merchants__mean_encoded','subsector_id_merchants__mean_encoded',\n",
    "    'category_1_merchants__mean_encoded','most_recent_sales_range_merchants__mean_encoded',\n",
    "    'most_recent_purchases_range_merchants__mean_encoded','category_4_merchants__mean_encoded',\n",
    "    'city_id_merchants__mean_encoded','state_id_merchants__mean_encoded',\n",
    "    'category_2_merchants__mean_encoded']\n",
    "\n",
    "# Features with 1 unique value that should be removed\n",
    "features_1_unique = [\n",
    "    \"purchase_date_year__min\",\"authorized_flag_-9999__mean\",\"authorized_flag_-9999__var\",\n",
    "    \"category_1_-9999__mean\",\"category_1_-9999__var\",\"active_months_lag3_merchants__max\",\n",
    "    \"active_months_lag6_merchants__max\"]\n",
    "\n",
    "# Features mode of IDs\n",
    "features_numeric_IDs_mode = [\n",
    "    'merchant_category_id__mode','subsector_id__mode','city_id__mode',\n",
    "    'state_id__mode','merchant_group_id_merchants__mode','merchant_category_id_merchants__mode',\n",
    "    'subsector_id_merchants__mode','city_id_merchants__mode','state_id_merchants__mode']\n",
    "\n",
    "features_categorical_IDs_mode = ['merchant_id__mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. For linear models data must be normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize numerical features, excluding the target and the mode of features IDs (which can be integer). *Note*: We will normalize all dataset (Train & Test) together but we could do it on Train and then apply the normalization on Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data,normalization_type='zscore',features_to_exclude=[]):\n",
    "    df = data.copy()\n",
    "    normalization = {}\n",
    "    for col in df.columns:\n",
    "        if data[col].dtype in [\"int\",\"float\",\"int64\",\"float64\",\"int32\",\"float32\"] and col not in features_to_exclude:\n",
    "            if(normalization_type == 'minmax'):\n",
    "                Min = df[col].min()\n",
    "                Max = df[col].max()\n",
    "                Dif = Max-Min\n",
    "                df[col] = df[col].apply(lambda x: (x-Min)/(Dif))\n",
    "                normalization[col] = ('minmax',Min,Max)\n",
    "            elif(normalization_type == 'zscore'):\n",
    "                Mean = df[col].mean()\n",
    "                Std = df[col].std()\n",
    "                df[col] = df[col].apply(lambda x: (x-Mean)/(Std))\n",
    "                normalization[col] = ('zscore',Mean,Std)\n",
    "    return(df,normalization)\n",
    "\n",
    "features_to_exclude = [target]+features_numeric_IDs_mode+features_categorical_IDs_mode\n",
    "data,normalization = normalization(data,'zscore',features_to_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Select features for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will exclude features with 1 unique value and all mode of ID features. Also, the mean encoded (since they do not give good RMSE Test results when submitting to Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((features_train_csv,features_transactions,features_mean_encoding))\n",
    "features = [col for col in features if col not in features_1_unique+features_categorical_IDs_mode+features_numeric_IDs_mode+features_mean_encoding]\n",
    "features_wo_mean_encoding = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Target metric: RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = lambda predictions,target: np.mean((predictions-target)**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split Train into 2 validation folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First separate Train & Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.loc[data[\"train_or_test\"]==\"train\",:]\n",
    "test = data.loc[data[\"train_or_test\"]==\"test\",:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then split Train into 2 validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folds(data,k=2,seed=1):\n",
    "    np.random.seed(seed)\n",
    "    data = data.iloc[np.random.permutation(data.index),:]\n",
    "    data_folds_list = [data.iloc[int(data.shape[0]*i/k):int(data.shape[0]*(i+1)/k),:] for i in range(k)]\n",
    "    return(data_folds_list)\n",
    "\n",
    "k = 2\n",
    "train_folds_list = folds(train,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Validation with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all parameters combinations for grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'NN': MLPRegressor,\n",
    "}\n",
    "\n",
    "parameters_names = {\n",
    "    'NN': ['hidden_layer_sizes','activation','solver','batch_size','learning_rate','learning_rate_init','max_iter','shuffle','random_state','momentum'],\n",
    "}\n",
    "\n",
    "parameters_values = {\n",
    "    # NN: 'hidden_layer_sizes','activation','solver',batch_size,learning_rate,learning_rate_init,max_iter,shuffle,random_state,momentum\n",
    "    'NN': [[(10),(15),(15,5),(5,15,5)],\n",
    "           ['logistic','tanh'],\n",
    "           ['sgd'],\n",
    "           [32,200,500],\n",
    "           ['constant'],\n",
    "           [0.01,0.05,0.1],\n",
    "           [500,1000],\n",
    "           [True],\n",
    "           [1],\n",
    "           [0.75,0.9]\n",
    "          ],\n",
    "}\n",
    "\n",
    "grid = {}\n",
    "for model in models.keys():\n",
    "    grid[model] = list(it.product(*parameters_values[model]))\n",
    "    for parametrization_index in range(len(grid[model])):\n",
    "        arguments = {}\n",
    "        for parameter_index, parameter_value in enumerate(grid[model][parametrization_index]):\n",
    "            arguments[parameters_names[model][parameter_index]] = parameter_value\n",
    "        grid[model][parametrization_index] = arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search: calulate the RMSE in validation splits for all parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs = {}\n",
    "for model in models.keys():\n",
    "    RMSEs[model] = pd.DataFrame(data=None,columns=np.concatenate((parameters_names[model],[\"Fold_\"+str(i) for i in range(k)],[\"Mean\",\"Std\"])))\n",
    "    for parametrization_index in range(len(grid[model])):\n",
    "        for fold in range(k):\n",
    "            # Define train and validation data for the ith fold\n",
    "            data_train = pd.concat([train_folds_list[i] for i in range(k) if i!=fold],axis=0)\n",
    "            data_val = train_folds_list[fold]        \n",
    "            x_train, y_train = data_train[features], data_train[target]\n",
    "            x_validation, y_validation = data_val[features], data_val[target]\n",
    "            # Train model\n",
    "            print(\"Parametrization \"+str(parametrization_index)+\" for Fold_\"+str(fold)+\" starts at \",datetime.datetime.now())\n",
    "            regressor = models[model](**grid[model][parametrization_index])\n",
    "            regressor.fit(x_train,y_train)\n",
    "            rmse_validation = rmse(regressor.predict(x_validation),y_validation)\n",
    "            print(\"and finishes at \",datetime.datetime.now(),\" with RMSE \",rmse_validation,\"\\n\")\n",
    "            # Add the RMSE to results\n",
    "            RMSEs[model].loc[parametrization_index,parameters_names[model]] = grid[model][parametrization_index]\n",
    "            RMSEs[model].loc[parametrization_index,\"Fold_\"+str(fold)] = rmse_validation\n",
    "        # Add the mean and std of the parametrization RMSE\n",
    "        RMSEs[model].loc[parametrization_index,\"Mean\"] = np.mean(RMSEs[model].loc[parametrization_index,[\"Fold_\"+str(i) for i in range(k)]])\n",
    "        RMSEs[model].loc[parametrization_index,\"Std\"] = np.std(RMSEs[model].loc[parametrization_index,[\"Fold_\"+str(i) for i in range(k)]])\n",
    "    # Sort the results by RMSE mean\n",
    "    RMSEs[model] = RMSEs[model].loc[np.argsort(RMSEs[model][\"Mean\"]),:]\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the validation results in a *pickle* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the results by RMSE mean\n",
    "RMSEs[model] = RMSEs[model].loc[np.argsort(RMSEs[model][\"Mean\"]),:]\n",
    "pickle.dump(RMSEs,open('Validations/Results_validation_NeuralNetwork.dat','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train optimal model with all Train and create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs = pd.read_pickle(\"Validations/Results_validation_NeuralNetwork.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs[\"NN\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submission_NeuralNetwork_parametrization_wo_mean_encoding.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all chosen parametrizations with all Train data. *Note:* We will use the 5 parametrizations with lowest mean RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"NN\"\n",
    "parametrization_indexes = RMSEs[model].index[0:5]\n",
    "# Define train data\n",
    "x_train, y_train = train[features_wo_mean_encoding], train[target]\n",
    "# Train model for all chosen parametrizations\n",
    "for parametrization_index in parametrization_indexes:\n",
    "    print(\"Parametrization \"+str(parametrization_index)+\" with all Train starts at \",datetime.datetime.now())\n",
    "    regressor = models[model](**grid[model][parametrization_index])\n",
    "    regressor.fit(x_train,y_train)\n",
    "    print(\"and finishes at \",datetime.datetime.now(),\"\\n\")\n",
    "    # Save model\n",
    "    pickle.dump(regressor,open('Models/Model_NeuralNetwork_parametrization'+str(parametrization_index)+'_wo_mean_encoding.dat','wb'))\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Test data and create *.csv* file for submitting to Kaggle for all chosen parametrizations without mean encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('Data/sample_submission.csv')\n",
    "print(\"Order of card_id in submission.csv OK?\",np.mean(submission[\"card_id\"].values==test[\"card_id\"].values)==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test[features_wo_mean_encoding]\n",
    "for parametrization_index in parametrization_indexes:\n",
    "    regressor = pickle.load(open('Models/Model_NeuralNetwork_parametrization'+str(parametrization_index)+'_wo_mean_encoding.dat','rb'))\n",
    "    submission[\"target\"] = regressor.predict(x_test)\n",
    "    submission.to_csv('Submissions/Submission_NeuralNetwork_parametrization'+str(parametrization_index)+'_wo_mean_encoding.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
