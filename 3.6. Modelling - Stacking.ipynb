{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import itertools as it\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"Data/final_data_with_mean_encoding.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lists of features by groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifiers\n",
    "identifiers = ['card_id','train_or_test','first_active_month']\n",
    "\n",
    "# Target\n",
    "target = 'target'\n",
    "\n",
    "# Features from original Train/Test.csv\n",
    "features_train_csv = ['feature_1','feature_2','feature_3','year','month','months_to_2018_02']\n",
    "\n",
    "# Features from all_transactions_with_merchants_groupby_cards\n",
    "features_transactions = [\n",
    "    'month_lag__mean','month_lag__std','month_lag__min','month_lag__max','month_lag__range',\n",
    "    'purchase_date__range',\n",
    "    'purchase_date__days_diff_next_purchase_mean','purchase_date__days_diff_next_purchase_std',\n",
    "    'purchase_date_year__mean','purchase_date_year__std','purchase_date_year__min','purchase_date_year__max','purchase_date_year__range',\n",
    "    'purchase_date_month__mean','purchase_date_month__std','purchase_date_month__min','purchase_date_month__max','purchase_date_month__range',\n",
    "    'purchase_date_day__mean','purchase_date_day__std','purchase_date_day__min','purchase_date_day__max','purchase_date_day__range',\n",
    "    'purchase_date_hour__mean','purchase_date_hour__std','purchase_date_hour__min','purchase_date_hour__max','purchase_date_hour__range',\n",
    "    'purchase_date_days_to_2018_02__mean','purchase_date_days_to_2018_02__std','purchase_date_days_to_2018_02__min','purchase_date_days_to_2018_02__max','purchase_date_days_to_2018_02__range',\n",
    "    'authorized_flag_N__mean','authorized_flag_N__var',\n",
    "    'authorized_flag_Y__mean','authorized_flag_Y__var',\n",
    "    'authorized_flag_-9999__mean','authorized_flag_-9999__var',\n",
    "    'category_3_A__mean','category_3_A__var',\n",
    "    'category_3_B__mean','category_3_B__var',\n",
    "    'category_3_C__mean','category_3_C__var',\n",
    "    'category_3_-9999__mean','category_3_-9999__var',\n",
    "    'installments__mean','installments__std','installments__min','installments__max','installments__range',\n",
    "    'category_1_N__mean','category_1_N__var',\n",
    "    'category_1_Y__mean','category_1_Y__var',\n",
    "    'category_1_-9999__mean','category_1_-9999__var',\n",
    "    'merchant_category_id__nunique','merchant_category_id__mode',\n",
    "    'subsector_id__nunique','subsector_id__mode',\n",
    "    'merchant_id__nunique','merchant_id__mode',\n",
    "    'purchase_amount__mean','purchase_amount__std','purchase_amount__min','purchase_amount__max','purchase_amount__range',\n",
    "    'city_id__nunique','city_id__mode',\n",
    "    'state_id__nunique','state_id__mode',\n",
    "    'category_2_1__mean','category_2_1__var',\n",
    "    'category_2_2__mean','category_2_2__var',\n",
    "    'category_2_3__mean','category_2_3__var',\n",
    "    'category_2_4__mean','category_2_4__var',\n",
    "    'category_2_5__mean','category_2_5__var',\n",
    "    'category_2_-9999__mean','category_2_-9999__var',\n",
    "    'merchant_group_id_merchants__nunique','merchant_group_id_merchants__mode',\n",
    "    'merchant_category_id_merchants__nunique','merchant_category_id_merchants__mode',\n",
    "    'subsector_id_merchants__nunique','subsector_id_merchants__mode',\n",
    "    'numerical_1_merchants__mean','numerical_1_merchants__std','numerical_1_merchants__min','numerical_1_merchants__max','numerical_1_merchants__range',\n",
    "    'numerical_2_merchants__mean','numerical_2_merchants__std','numerical_2_merchants__min','numerical_2_merchants__max','numerical_2_merchants__range',\n",
    "    'category_1_merchants_N__mean','category_1_merchants_N__var',\n",
    "    'category_1_merchants_Y__mean','category_1_merchants_Y__var',\n",
    "    'category_1_merchants_-9999__mean','category_1_merchants_-9999__var',\n",
    "    'most_recent_sales_range_merchants_A__mean','most_recent_sales_range_merchants_A__var',\n",
    "    'most_recent_sales_range_merchants_B__mean','most_recent_sales_range_merchants_B__var',\n",
    "    'most_recent_sales_range_merchants_C__mean','most_recent_sales_range_merchants_C__var',\n",
    "    'most_recent_sales_range_merchants_D__mean','most_recent_sales_range_merchants_D__var',\n",
    "    'most_recent_sales_range_merchants_E__mean','most_recent_sales_range_merchants_E__var',\n",
    "    'most_recent_sales_range_merchants_-9999__mean','most_recent_sales_range_merchants_-9999__var',\n",
    "    'most_recent_purchases_range_merchants_A__mean','most_recent_purchases_range_merchants_A__var',\n",
    "    'most_recent_purchases_range_merchants_B__mean','most_recent_purchases_range_merchants_B__var',\n",
    "    'most_recent_purchases_range_merchants_C__mean','most_recent_purchases_range_merchants_C__var',\n",
    "    'most_recent_purchases_range_merchants_D__mean','most_recent_purchases_range_merchants_D__var',\n",
    "    'most_recent_purchases_range_merchants_E__mean','most_recent_purchases_range_merchants_E__var',\n",
    "    'most_recent_purchases_range_merchants_-9999__mean','most_recent_purchases_range_merchants_-9999__var',\n",
    "    'avg_sales_lag3_merchants__mean','avg_sales_lag3_merchants__std','avg_sales_lag3_merchants__min','avg_sales_lag3_merchants__max','avg_sales_lag3_merchants__range',\n",
    "    'avg_purchases_lag3_merchants__mean','avg_purchases_lag3_merchants__std','avg_purchases_lag3_merchants__min','avg_purchases_lag3_merchants__max','avg_purchases_lag3_merchants__range',\n",
    "    'active_months_lag3_merchants__mean','active_months_lag3_merchants__std','active_months_lag3_merchants__min','active_months_lag3_merchants__max','active_months_lag3_merchants__range',\n",
    "    'avg_sales_lag6_merchants__mean','avg_sales_lag6_merchants__std','avg_sales_lag6_merchants__min','avg_sales_lag6_merchants__max','avg_sales_lag6_merchants__range',\n",
    "    'avg_purchases_lag6_merchants__mean','avg_purchases_lag6_merchants__std','avg_purchases_lag6_merchants__min','avg_purchases_lag6_merchants__max','avg_purchases_lag6_merchants__range',\n",
    "    'active_months_lag6_merchants__mean','active_months_lag6_merchants__std','active_months_lag6_merchants__min','active_months_lag6_merchants__max','active_months_lag6_merchants__range',\n",
    "    'avg_sales_lag12_merchants__mean','avg_sales_lag12_merchants__std','avg_sales_lag12_merchants__min','avg_sales_lag12_merchants__max','avg_sales_lag12_merchants__range',\n",
    "    'avg_purchases_lag12_merchants__mean','avg_purchases_lag12_merchants__std','avg_purchases_lag12_merchants__min','avg_purchases_lag12_merchants__max','avg_purchases_lag12_merchants__range',\n",
    "    'active_months_lag12_merchants__mean','active_months_lag12_merchants__std','active_months_lag12_merchants__min','active_months_lag12_merchants__max','active_months_lag12_merchants__range',\n",
    "    'category_4_merchants_N__mean','category_4_merchants_N__var',\n",
    "    'category_4_merchants_Y__mean','category_4_merchants_Y__var',\n",
    "    'category_4_merchants_-9999__mean','category_4_merchants_-9999__var',\n",
    "    'city_id_merchants__nunique','city_id_merchants__mode',\n",
    "    'state_id_merchants__nunique','state_id_merchants__mode',\n",
    "    'category_2_merchants_1__mean','category_2_merchants_1__var',\n",
    "    'category_2_merchants_2__mean','category_2_merchants_2__var',\n",
    "    'category_2_merchants_3__mean','category_2_merchants_3__var',\n",
    "    'category_2_merchants_4__mean','category_2_merchants_4__var',\n",
    "    'category_2_merchants_5__mean','category_2_merchants_5__var',\n",
    "    'category_2_merchants_-9999__mean','category_2_merchants_-9999__var']\n",
    "\n",
    "# Features from mean_encoding_all_transactions_with_merchants_groupby_card\n",
    "features_mean_encoding = [\n",
    "    'authorized_flag__mean_encoded','category_3__mean_encoded','category_1__mean_encoded',\n",
    "    'merchant_category_id__mean_encoded','subsector_id__mean_encoded',\n",
    "    'merchant_id__mean_encoded','city_id__mean_encoded','state_id__mean_encoded',\n",
    "    'category_2__mean_encoded','merchant_group_id_merchants__mean_encoded',\n",
    "    'merchant_category_id_merchants__mean_encoded','subsector_id_merchants__mean_encoded',\n",
    "    'category_1_merchants__mean_encoded','most_recent_sales_range_merchants__mean_encoded',\n",
    "    'most_recent_purchases_range_merchants__mean_encoded','category_4_merchants__mean_encoded',\n",
    "    'city_id_merchants__mean_encoded','state_id_merchants__mean_encoded',\n",
    "    'category_2_merchants__mean_encoded']\n",
    "\n",
    "# Features with 1 unique value that should be removed\n",
    "features_1_unique = [\n",
    "    \"purchase_date_year__min\",\"authorized_flag_-9999__mean\",\"authorized_flag_-9999__var\",\n",
    "    \"category_1_-9999__mean\",\"category_1_-9999__var\",\"active_months_lag3_merchants__max\",\n",
    "    \"active_months_lag6_merchants__max\"]\n",
    "\n",
    "# Features mode of IDs\n",
    "features_numeric_IDs_mode = [\n",
    "    'merchant_category_id__mode','subsector_id__mode','city_id__mode',\n",
    "    'state_id__mode','merchant_group_id_merchants__mode','merchant_category_id_merchants__mode',\n",
    "    'subsector_id_merchants__mode','city_id_merchants__mode','state_id_merchants__mode']\n",
    "\n",
    "features_categorical_IDs_mode = ['merchant_id__mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. For linear models data must be normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize numerical features, excluding the target and the mode of features IDs (which can be integer). *Note*: We will normalize all dataset (Train & Test) together but we could do it on Train and then apply the normalization on Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data,normalization_type='zscore',features_to_exclude=[]):\n",
    "    df = data.copy()\n",
    "    normalization = {}\n",
    "    for col in df.columns:\n",
    "        if data[col].dtype in [\"int\",\"float\",\"int64\",\"float64\",\"int32\",\"float32\"] and col not in features_to_exclude:\n",
    "            if(normalization_type == 'minmax'):\n",
    "                Min = df[col].min()\n",
    "                Max = df[col].max()\n",
    "                Dif = Max-Min\n",
    "                df[col] = df[col].apply(lambda x: (x-Min)/(Dif))\n",
    "                normalization[col] = ('minmax',Min,Max)\n",
    "            elif(normalization_type == 'zscore'):\n",
    "                Mean = df[col].mean()\n",
    "                Std = df[col].std()\n",
    "                df[col] = df[col].apply(lambda x: (x-Mean)/(Std))\n",
    "                normalization[col] = ('zscore',Mean,Std)\n",
    "    return(df,normalization)\n",
    "\n",
    "features_to_exclude = [target]+features_numeric_IDs_mode+features_categorical_IDs_mode\n",
    "sdata_linear,normalization = normalization(data,'zscore',features_to_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. For non-linear models data must be int, float or bool\n",
    "\n",
    "Let's modify features type category and object except *merchant_id__mode* which is the unique ID feature no numerical, and we will simply not include it in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_non_linear = data.copy()\n",
    "features_to_modify = [\"feature_1\",\"feature_2\",\"feature_3\"]+features_numeric_IDs_mode\n",
    "for col in features_to_modify:\n",
    "    data_non_linear[col] = data_non_linear[col].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Select features for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use same features than used for training saved models in other notebooks. We will exclude features with 1 unique value, the mean encoded (since they do not give good RMSE Test results when submitting to Kaggle), and:\n",
    "- For linear models: all mode of ID features\n",
    "- For non-linear models: ID feature *merchant_id__mode*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((features_train_csv,features_transactions,features_mean_encoding))\n",
    "features_linear = [col for col in features if col not in features_1_unique+features_categorical_IDs_mode+features_numeric_IDs_mode+features_mean_encoding]\n",
    "features_non_linear = [col for col in features if col not in features_1_unique+features_categorical_IDs_mode+features_mean_encoding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Split Train & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First separate Train & Test (for linear and non-linear models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear models\n",
    "train_linear = data_linear.loc[data_linear[\"train_or_test\"]==\"train\",:]\n",
    "test_linear = data_linear.loc[data_linear[\"train_or_test\"]==\"test\",:]\n",
    "# Non-linear models\n",
    "train_non_linear = data_non_linear.loc[data_non_linear[\"train_or_test\"]==\"train\",:]\n",
    "test_non_linear = data_non_linear.loc[data_non_linear[\"train_or_test\"]==\"test\",:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Stacking several saved models and create submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the path of saved models you want to include in stacking, as well as the Train & Test data for each one (linear or non-linear model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'Models/Model_XGBoost_parametrization297_wo_mean_encoding.dat',\n",
    "    'Models/Model_XGBoost_parametrization453_wo_mean_encoding.dat',\n",
    "    'Models/Model_XGBoost_parametrization457_wo_mean_encoding.dat',\n",
    "    'Models/Model_XGBoost_parametrization461_wo_mean_encoding.dat',\n",
    "    'Models/Model_XGBoost_parametrization613_wo_mean_encoding.dat',\n",
    "]\n",
    "\n",
    "train_data_models = {\n",
    "    'Models/Model_XGBoost_parametrization297_wo_mean_encoding.dat': train_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization453_wo_mean_encoding.dat': train_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization457_wo_mean_encoding.dat': train_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization461_wo_mean_encoding.dat': train_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization613_wo_mean_encoding.dat': train_non_linear,\n",
    "}\n",
    "\n",
    "test_data_models = {\n",
    "    'Models/Model_XGBoost_parametrization297_wo_mean_encoding.dat': test_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization453_wo_mean_encoding.dat': test_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization457_wo_mean_encoding.dat': test_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization461_wo_mean_encoding.dat': test_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization613_wo_mean_encoding.dat': test_non_linear,\n",
    "}\n",
    "\n",
    "features_models = {\n",
    "    'Models/Model_XGBoost_parametrization297_wo_mean_encoding.dat': features_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization453_wo_mean_encoding.dat': features_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization457_wo_mean_encoding.dat': features_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization461_wo_mean_encoding.dat': features_non_linear,\n",
    "    'Models/Model_XGBoost_parametrization613_wo_mean_encoding.dat': features_non_linear,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Train & Test predictions with all saved models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = pd.DataFrame()\n",
    "test_predictions = pd.DataFrame()\n",
    "for model in models:\n",
    "    regressor = pickle.load(open(model,'rb'))\n",
    "    train_predictions[model] = regressor.predict(train_data_models[model][features_models[model]])\n",
    "    test_predictions[model] = regressor.predict(test_data_models[model][features_models[model]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Linear Regression as metalearner or 2nd stacking layer model on the Train predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalearner = LinearRegression(fit_intercept=True,normalize=False)\n",
    "metalearner.fit(train_predictions,train_linear[target])\n",
    "# Save model\n",
    "pickle.dump(metalearner,open('Models/Model_Stacking.dat','wb'))\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Test data (using metalearner and Test predictions) and create *.csv* file for submitting to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('Data/sample_submission.csv')\n",
    "print(\"Order of card_id in submission.csv OK?\",np.mean(submission[\"card_id\"].values==test_linear[\"card_id\"].values)==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_predictions\n",
    "metalearner = pickle.load(open('Models/Model_Stacking.dat','rb'))\n",
    "submission[\"target\"] = metalearner.predict(x_test)\n",
    "submission.to_csv('Submissions/Submission_Model_Stacking.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data_non_linear.head()\n",
    "data_linear.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
