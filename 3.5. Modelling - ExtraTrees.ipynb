{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import itertools as it\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"Data/final_data_with_mean_encoding.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lists of features by groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifiers\n",
    "identifiers = ['card_id','train_or_test','first_active_month']\n",
    "\n",
    "# Target\n",
    "target = 'target'\n",
    "\n",
    "# Features from original Train/Test.csv\n",
    "features_train_csv = ['feature_1','feature_2','feature_3','year','month','months_to_2018_02']\n",
    "\n",
    "# Features from all_transactions_merchants_groupby_cards\n",
    "features_transactions = [\n",
    "'month_lag__mean','month_lag__std','month_lag__min','month_lag__max','month_lag__range',\n",
    "'purchase_date__range','purchase_date__days_diff_next_purchase_mean',\n",
    "'purchase_date__days_diff_next_purchase_std','purchase_date_year__mean','purchase_date_year__std',\n",
    "'purchase_date_year__min','purchase_date_year__max','purchase_date_year__range',\n",
    "'purchase_date_month__mean','purchase_date_month__std','purchase_date_month__min',\n",
    "'purchase_date_month__max','purchase_date_month__range','purchase_date_day__mean',\n",
    "'purchase_date_day__std','purchase_date_day__min','purchase_date_day__max','purchase_date_day__range',\n",
    "'purchase_date_hour__mean','purchase_date_hour__std','purchase_date_hour__min','purchase_date_hour__max',\n",
    "'purchase_date_hour__range','purchase_date_days_to_2018_02__mean','purchase_date_days_to_2018_02__std',\n",
    "'purchase_date_days_to_2018_02__min','purchase_date_days_to_2018_02__max',\n",
    "'purchase_date_days_to_2018_02__range','authorized_flag_N__mean','authorized_flag_N__var',\n",
    "'authorized_flag_Y__mean','authorized_flag_Y__var','authorized_flag_-9999__mean',\n",
    "'authorized_flag_-9999__var','category_3_A__mean','category_3_A__var','category_3_B__mean',\n",
    "'category_3_B__var','category_3_C__mean','category_3_C__var','category_3_-9999__mean',\n",
    "'category_3_-9999__var','installments__mean','installments__std','installments__min','installments__max',\n",
    "'installments__range','category_1_N__mean','category_1_N__var','category_1_Y__mean','category_1_Y__var',\n",
    "'category_1_-9999__mean','category_1_-9999__var','merchant_category_id__nunique',\n",
    "'merchant_category_id__mode','subsector_id__nunique','subsector_id__mode','merchant_id__nunique',\n",
    "'merchant_id__mode','purchase_amount__mean','purchase_amount__std','purchase_amount__min',\n",
    "'purchase_amount__max','purchase_amount__range','city_id__nunique','city_id__mode','state_id__nunique',\n",
    "'state_id__mode','category_2_1__mean','category_2_1__var','category_2_2__mean','category_2_2__var',\n",
    "'category_2_3__mean','category_2_3__var','category_2_4__mean','category_2_4__var','category_2_5__mean',\n",
    "'category_2_5__var','category_2_-9999__mean','category_2_-9999__var',\n",
    "'merchant_group_id_merchants__nunique','merchant_group_id_merchants__mode',\n",
    "'merchant_category_id_merchants__nunique','merchant_category_id_merchants__mode',\n",
    "'subsector_id_merchants__nunique','subsector_id_merchants__mode','numerical_1_merchants__mean',\n",
    "'numerical_1_merchants__std','numerical_1_merchants__min','numerical_1_merchants__max',\n",
    "'numerical_1_merchants__range','numerical_2_merchants__mean','numerical_2_merchants__std',\n",
    "'numerical_2_merchants__min','numerical_2_merchants__max','numerical_2_merchants__range',\n",
    "'category_1_merchants_N__mean','category_1_merchants_N__var','category_1_merchants_Y__mean',\n",
    "'category_1_merchants_Y__var','category_1_merchants_-9999__mean','category_1_merchants_-9999__var',\n",
    "'most_recent_sales_range_merchants_A__mean','most_recent_sales_range_merchants_A__var',\n",
    "'most_recent_sales_range_merchants_B__mean','most_recent_sales_range_merchants_B__var',\n",
    "'most_recent_sales_range_merchants_C__mean','most_recent_sales_range_merchants_C__var',\n",
    "'most_recent_sales_range_merchants_D__mean','most_recent_sales_range_merchants_D__var',\n",
    "'most_recent_sales_range_merchants_E__mean','most_recent_sales_range_merchants_E__var',\n",
    "'most_recent_sales_range_merchants_-9999__mean','most_recent_sales_range_merchants_-9999__var',\n",
    "'most_recent_purchases_range_merchants_A__mean','most_recent_purchases_range_merchants_A__var',\n",
    "'most_recent_purchases_range_merchants_B__mean','most_recent_purchases_range_merchants_B__var',\n",
    "'most_recent_purchases_range_merchants_C__mean','most_recent_purchases_range_merchants_C__var',\n",
    "'most_recent_purchases_range_merchants_D__mean','most_recent_purchases_range_merchants_D__var',\n",
    "'most_recent_purchases_range_merchants_E__mean','most_recent_purchases_range_merchants_E__var',\n",
    "'most_recent_purchases_range_merchants_-9999__mean','most_recent_purchases_range_merchants_-9999__var',\n",
    "'avg_sales_lag3_merchants__mean','avg_sales_lag3_merchants__std','avg_sales_lag3_merchants__min',\n",
    "'avg_sales_lag3_merchants__max','avg_sales_lag3_merchants__range','avg_purchases_lag3_merchants__mean',\n",
    "'avg_purchases_lag3_merchants__std','avg_purchases_lag3_merchants__min',\n",
    "'avg_purchases_lag3_merchants__max','avg_purchases_lag3_merchants__range',\n",
    "'active_months_lag3_merchants__mean','active_months_lag3_merchants__std',\n",
    "'active_months_lag3_merchants__min','active_months_lag3_merchants__max',\n",
    "'active_months_lag3_merchants__range','avg_sales_lag6_merchants__mean','avg_sales_lag6_merchants__std',\n",
    "'avg_sales_lag6_merchants__min','avg_sales_lag6_merchants__max','avg_sales_lag6_merchants__range',\n",
    "'avg_purchases_lag6_merchants__mean','avg_purchases_lag6_merchants__std',\n",
    "'avg_purchases_lag6_merchants__min','avg_purchases_lag6_merchants__max',\n",
    "'avg_purchases_lag6_merchants__range','active_months_lag6_merchants__mean',\n",
    "'active_months_lag6_merchants__std','active_months_lag6_merchants__min',\n",
    "'active_months_lag6_merchants__max','active_months_lag6_merchants__range',\n",
    "'avg_sales_lag12_merchants__mean','avg_sales_lag12_merchants__std','avg_sales_lag12_merchants__min',\n",
    "'avg_sales_lag12_merchants__max','avg_sales_lag12_merchants__range',\n",
    "'avg_purchases_lag12_merchants__mean','avg_purchases_lag12_merchants__std',\n",
    "'avg_purchases_lag12_merchants__min','avg_purchases_lag12_merchants__max',\n",
    "'avg_purchases_lag12_merchants__range','active_months_lag12_merchants__mean',\n",
    "'active_months_lag12_merchants__std','active_months_lag12_merchants__min',\n",
    "'active_months_lag12_merchants__max','active_months_lag12_merchants__range','category_4_merchants_N__mean',\n",
    "'category_4_merchants_N__var','category_4_merchants_Y__mean','category_4_merchants_Y__var',\n",
    "'category_4_merchants_-9999__mean','category_4_merchants_-9999__var','city_id_merchants__nunique',\n",
    "'city_id_merchants__mode','state_id_merchants__nunique','state_id_merchants__mode',\n",
    "'category_2_merchants_1__mean','category_2_merchants_1__var','category_2_merchants_2__mean',\n",
    "'category_2_merchants_2__var','category_2_merchants_3__mean','category_2_merchants_3__var',\n",
    "'category_2_merchants_4__mean','category_2_merchants_4__var','category_2_merchants_5__mean',\n",
    "'category_2_merchants_5__var','category_2_merchants_-9999__mean','category_2_merchants_-9999__var']\n",
    "\n",
    "# Features from mean_encoding_all_transactions_merchants_groupby_card\n",
    "features_mean_encoding = [\n",
    "    'authorized_flag__mean_encoded','category_3__mean_encoded','category_1__mean_encoded',\n",
    "    'merchant_category_id__mean_encoded','subsector_id__mean_encoded',\n",
    "    'merchant_id__mean_encoded','city_id__mean_encoded','state_id__mean_encoded',\n",
    "    'category_2__mean_encoded','merchant_group_id_merchants__mean_encoded',\n",
    "    'merchant_category_id_merchants__mean_encoded','subsector_id_merchants__mean_encoded',\n",
    "    'category_1_merchants__mean_encoded','most_recent_sales_range_merchants__mean_encoded',\n",
    "    'most_recent_purchases_range_merchants__mean_encoded','category_4_merchants__mean_encoded',\n",
    "    'city_id_merchants__mean_encoded','state_id_merchants__mean_encoded',\n",
    "    'category_2_merchants__mean_encoded']\n",
    "\n",
    "# Features with 1 unique value that should be removed\n",
    "features_1_unique = [\n",
    "    \"purchase_date_year__min\",\"authorized_flag_-9999__mean\",\"authorized_flag_-9999__var\",\n",
    "    \"category_1_-9999__mean\",\"category_1_-9999__var\",\"active_months_lag3_merchants__max\",\n",
    "    \"active_months_lag6_merchants__max\"]\n",
    "\n",
    "# Features mode of IDs\n",
    "features_numeric_IDs_mode = [\n",
    "    'merchant_category_id__mode','subsector_id__mode','city_id__mode',\n",
    "    'state_id__mode','merchant_group_id_merchants__mode','merchant_category_id_merchants__mode',\n",
    "    'subsector_id_merchants__mode','city_id_merchants__mode','state_id_merchants__mode']\n",
    "\n",
    "features_categorical_IDs_mode = ['merchant_id__mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. For ExtraTreesRegressor data must be int, float or bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify features type category and object except *merchant_id__mode* which is the unique ID feature no numerical, and we will simply not include it in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_modify = [\"feature_1\",\"feature_2\",\"feature_3\"]+features_numeric_IDs_mode\n",
    "for col in features_to_modify:\n",
    "    data[col] = data[col].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Select features for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will exclude features with 1 unique value and *merchant_id__mode*. Also, the mean encoded (since they do not give good RMSE Test results when submitting to Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((features_train_csv,features_transactions,features_mean_encoding))\n",
    "features = [col for col in features if col not in features_1_unique+features_categorical_IDs_mode+\\\n",
    "            features_mean_encoding]\n",
    "features_wo_mean_encoding = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Target metric: RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = lambda predictions,target: np.mean((predictions-target)**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split Train into 2 validation folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First separate Train & Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.loc[data[\"train_or_test\"]==\"train\",:]\n",
    "test = data.loc[data[\"train_or_test\"]==\"test\",:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then split Train into 2 validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folds(data,k=2,seed=1):\n",
    "    np.random.seed(seed)\n",
    "    data = data.iloc[np.random.permutation(data.index),:]\n",
    "    data_folds_list = [data.iloc[int(data.shape[0]*i/k):int(data.shape[0]*(i+1)/k),:] for i in range(k)]\n",
    "    return(data_folds_list)\n",
    "\n",
    "k = 2\n",
    "train_folds_list = folds(train,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Validation with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all parameters combinations for grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'ET': ExtraTreesRegressor,\n",
    "}\n",
    "\n",
    "parameters_names = {\n",
    "    'ET': ['n_estimators','criterion','max_depth','min_samples_leaf','max_features',\n",
    "           'min_impurity_decrease','bootstrap','n_jobs','random_state'],\n",
    "}\n",
    "\n",
    "parameters_values = {\n",
    "    # ET: n_estimators, criterion, max_depth, min_samples_leaf, max_features, min_impurity_decrease,\n",
    "    #     bootstrap, n_jobs, random_state\n",
    "    'ET': [[100,250,500],\n",
    "           [\"mse\"],\n",
    "           [15,20,25],\n",
    "           [10,20,30,40],\n",
    "           [\"auto\",\"sqrt\"],\n",
    "           [0,0.1],\n",
    "           [0.75,1],\n",
    "           [6],\n",
    "           [1]\n",
    "          ],\n",
    "}\n",
    "\n",
    "grid = {}\n",
    "for model in models.keys():\n",
    "    grid[model] = list(it.product(*parameters_values[model]))\n",
    "    for idx in range(len(grid[model])):\n",
    "        arguments = {}\n",
    "        for parameter_index, parameter_value in enumerate(grid[model][idx]):\n",
    "            arguments[parameters_names[model][parameter_index]] = parameter_value\n",
    "        grid[model][idx] = arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search: calulate the RMSE in validation splits for all parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs = {}\n",
    "for model in models.keys():\n",
    "    RMSEs[model] = pd.DataFrame(data=None,columns=parameters_names[model]+[\"Fold_\"+str(i) for i in \\\n",
    "                                                                           range(k)+[\"Mean\",\"Std\"])\n",
    "    for idx in range(len(grid[model])):\n",
    "        for fold in range(k):\n",
    "            # Define train and validation data for the ith fold\n",
    "            data_train = pd.concat([train_folds_list[i] for i in range(k) if i!=fold],axis=0)\n",
    "            data_val = train_folds_list[fold]        \n",
    "            x_train, y_train = data_train[features], data_train[target]\n",
    "            x_validation, y_validation = data_val[features], data_val[target]\n",
    "            # Train model\n",
    "            print(\"Parametrization \"+str(idx)+\" for Fold_\"+str(fold)+\" starts at \",\n",
    "                  datetime.datetime.now())\n",
    "            regressor = models[model](**grid[model][idx])\n",
    "            regressor.fit(x_train,y_train)\n",
    "            rmse_validation = rmse(regressor.predict(x_validation),y_validation)\n",
    "            print(\"and finishes at \",datetime.datetime.now(),\" with RMSE \",rmse_validation,\"\\n\")\n",
    "            # Add the RMSE to results\n",
    "            RMSEs[model].loc[idx,parameters_names[model]] = grid[model][idx]\n",
    "            RMSEs[model].loc[idx,\"Fold_\"+str(fold)] = rmse_validation\n",
    "        # Add the mean and std of the parametrization RMSE\n",
    "        RMSEs[model].loc[idx,\"Mean\"] = np.mean(RMSEs[model].loc[idx,[\"Fold_\"+str(i) for i in range(k)]])\n",
    "        RMSEs[model].loc[idx,\"Std\"] = np.std(RMSEs[model].loc[idx,[\"Fold_\"+str(i) for i in range(k)]])\n",
    "    # Sort the results by RMSE mean\n",
    "    RMSEs[model] = RMSEs[model].loc[np.argsort(RMSEs[model][\"Mean\"]),:]\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the validation results in a *pickle* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the results by RMSE mean\n",
    "RMSEs[model] = RMSEs[model].loc[np.argsort(RMSEs[model][\"Mean\"]),:]\n",
    "pickle.dump(RMSEs,open('Validations/Results_validation_ExtraTrees.dat','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train optimal model with all Train and create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs = pd.read_pickle(\"Validations/Results_validation_ExtraTrees.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_features</th>\n",
       "      <th>min_impurity_decrease</th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>random_state</th>\n",
       "      <th>Fold_0</th>\n",
       "      <th>Fold_1</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>500</td>\n",
       "      <td>mse</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.91333</td>\n",
       "      <td>3.77405</td>\n",
       "      <td>3.84369</td>\n",
       "      <td>0.0696371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>500</td>\n",
       "      <td>mse</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.91333</td>\n",
       "      <td>3.77405</td>\n",
       "      <td>3.84369</td>\n",
       "      <td>0.0696371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>500</td>\n",
       "      <td>mse</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.91333</td>\n",
       "      <td>3.77405</td>\n",
       "      <td>3.84369</td>\n",
       "      <td>0.0696371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>500</td>\n",
       "      <td>mse</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.83292</td>\n",
       "      <td>3.70276</td>\n",
       "      <td>3.76784</td>\n",
       "      <td>0.065081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>250</td>\n",
       "      <td>mse</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>auto</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.78358</td>\n",
       "      <td>3.65765</td>\n",
       "      <td>3.72062</td>\n",
       "      <td>0.0629617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators criterion max_depth min_samples_leaf max_features  \\\n",
       "271          500       mse        25               20         sqrt   \n",
       "270          500       mse        25               20         sqrt   \n",
       "278          500       mse        25               30         sqrt   \n",
       "245          500       mse        20               30         sqrt   \n",
       "105          250       mse        15               20         auto   \n",
       "\n",
       "    min_impurity_decrease bootstrap n_jobs random_state   Fold_0   Fold_1  \\\n",
       "271                   0.1         1      6            1  3.91333  3.77405   \n",
       "270                   0.1      0.75      6            1  3.91333  3.77405   \n",
       "278                   0.1      0.75      6            1  3.91333  3.77405   \n",
       "245                     0         1      6            1  3.83292  3.70276   \n",
       "105                     0         1      6            1  3.78358  3.65765   \n",
       "\n",
       "        Mean        Std  \n",
       "271  3.84369  0.0696371  \n",
       "270  3.84369  0.0696371  \n",
       "278  3.84369  0.0696371  \n",
       "245  3.76784   0.065081  \n",
       "105  3.72062  0.0629617  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSEs[\"ET\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submission_ExtraTrees_parametrization_wo_mean_encoding.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all chosen parametrizations with all Train data. *Note:* We will use the 5 parametrizations with lowest mean RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametrization 271 with all Train starts at  2019-02-19 20:42:03.323883\n",
      "and finishes at  2019-02-19 20:42:07.306427 \n",
      "\n",
      "Parametrization 270 with all Train starts at  2019-02-19 20:42:07.318395\n",
      "and finishes at  2019-02-19 20:42:11.727976 \n",
      "\n",
      "Parametrization 278 with all Train starts at  2019-02-19 20:42:11.738915\n",
      "and finishes at  2019-02-19 20:42:15.721446 \n",
      "\n",
      "Parametrization 245 with all Train starts at  2019-02-19 20:42:15.734440\n",
      "and finishes at  2019-02-19 20:43:23.951736 \n",
      "\n",
      "Parametrization 105 with all Train starts at  2019-02-19 20:43:24.450096\n",
      "and finishes at  2019-02-19 20:49:09.258082 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"ET\"\n",
    "parametrization_indexes = RMSEs[model].index[0:5]\n",
    "# Define train data\n",
    "x_train, y_train = train[features_wo_mean_encoding], train[target]\n",
    "# Train model for all chosen parametrizations\n",
    "for idx in parametrization_indexes:\n",
    "    print(\"Parametrization \"+str(idx)+\" with all Train starts at \",datetime.datetime.now())\n",
    "    regressor = models[model](**grid[model][idx])\n",
    "    regressor.fit(x_train,y_train)\n",
    "    print(\"and finishes at \",datetime.datetime.now(),\"\\n\")\n",
    "    # Save model\n",
    "    pickle.dump(regressor,open('Models/Model_ExtraTrees_parametrization'+str(idx)+'_wo_mean_encoding.dat',\n",
    "                               'wb'))\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Test data and create *.csv* file for submitting to Kaggle for all chosen parametrizations without mean encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of card_id in submission.csv OK? True\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('Data/sample_submission.csv')\n",
    "print(\"Order of card_id in submission.csv OK?\",np.mean(submission[\"card_id\"].values==test[\"card_id\"].\\\n",
    "                                                       values)==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test[features_wo_mean_encoding]\n",
    "for idx in parametrization_indexes:\n",
    "    regressor = pickle.load(open('Models/Model_ExtraTrees_parametrization'+str(idx)+\n",
    "                                 '_wo_mean_encoding.dat','rb'))\n",
    "    submission[\"target\"] = regressor.predict(x_test)\n",
    "    submission.to_csv('Submissions/Submission_ExtraTrees_parametrization'+str(idx)+\n",
    "                      '_wo_mean_encoding.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
